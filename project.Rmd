---
title: "Project"
author: "Group 3"
date: "2025-04-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Cholesterol Project

```{r data, libraries}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(leaps)
library(nortest)
library(car)
library(ggcorrplot)
library(gridExtra)

df<-read_csv("CHOLESTEROL.csv")
df
```
## BMI
```{r BMI}
df$BMI <- (df$Weight / (df$Height^2)) * 703
df<-df %>% dplyr::select(-c(Weight, Height))
df <- df %>% dplyr::select(Tot_Chol, everything())
df
```

## Demographics
```{r demographics}
pie.gender<-ggplot(df, aes())

pie.heredity<-ggplot(df, aes(x="",fill=as.factor(Heredity),group=as.factor(Heredity)))+
  geom_bar(position="fill")+
  geom_text(stat='count', 
            aes(y=after_stat(..count..),label=after_stat(..count..)),
            position=position_fill(0.5))+
  coord_polar(theta="y") +
  theme_void()+
  labs(title="Percentages of Hypercholesterolemia Heredity",fill="Number of Parents with Hypercholesterolemia")


pie.smoke<-ggplot()
  
pie.race<-ggplot()

pie.diet<-ggplot()
  
pie.alcohol<-ggplot()

histogram.age<-ggplot()
  
dot.exercise<-ggplot()
  
dot.sleep<-ggplot()
  
histogram.bmi<-ggplot()
  
histogram.chol<-ggplot()
```

### Interpretations
stuff idk

## Correlations
```{r correlation}
# correlation matrix

# scatter plot thingy
```

### Interpretations
Total Cholesterol is highly correlated with xyz.

## Recode Categorical Variables
```{r recode}
df2<-df%>%dplyr::select(Tot_Chol, BMI, Diet, Age, Sleep, Exercise)

# Gender where 1 is male
df2$Gender<-ifelse(df$Gender=="Male",1,0)
# H1 = 1 parent, H2 = 2 parents with Hypercholesterolemia
df2$H1<-ifelse(df$Heredity == 1, 1, 0)
df2$H2<-ifelse(df$Heredity == 2, 1, 0)
# Smoke where S1 is light smoker, and S2 is heavy
df2$S1<-ifelse(df$Smoke==1,1,0)
df2$S2<-ifelse(df$Smoke==2,1,0)
# Race where R1 is Hispanic, R2 is Black, and R3 is Asian
df2$R1<-ifelse(df$Race==1,1,0)
df2$R2<-ifelse(df$Race==2,1,0)
df2$R3<-ifelse(df$Race==3,1,0)
# Alcohol where A1 is social drinker, and A2 is heavy
df2$A1<-ifelse(df$Alcohol==1,1,0)
df2$A2<-ifelse(df$Alcohol==2,1,0)
df2
```

## Full Model
```{r full model}
full_model <- lm(Tot_Chol ~., data = df2)
summary(full_model)

par(mfrow = c(3, 2))
# Create a residual plot
plot(full_model)
qqnorm(full_model$residuals)
qqline(full_model$residuals)
ad.test(full_model$residuals)
vif(full_model)
```

### Interpretations
Assumptions for a linear model are met as the residuals appear to have equal variance and there is not sufficient evidence to show they are not normal (Anderson-Darling p-value=0.7529). The full model is too expensive as it uses 10+ variables, so we will conduct best subsets. Appears to be no problems with co-linearity.

## Best Subsets
```{r subsets}
subsets <- regsubsets(Tot_Chol ~., data = df2, nbest = 2)
subset.summary<-summary(subsets)

RSQ<-round(subset.summary$rsq * 100, digits=2)
ADJ_RSQ<-round(subset.summary$adjr2 * 100, digits=2)
CP<-round(subset.summary$cp, digits=2)
BIC<-round(subset.summary$bic, digits=2)
b<-subset.summary$outmat
Nvar<-1:nrow(subset.summary$which)
a<-cbind(Nvar,b,RSQ,ADJ_RSQ,CP,BIC)
a<-as.data.frame(a)
a$Nvar<-as.numeric(a$Nvar)
a$RSQ<-as.numeric(a$RSQ)
a$ADJ_RSQ<-as.numeric(a$ADJ_RSQ)
a$CP<-as.numeric(a$CP)
a$BIC<-as.numeric(a$BIC)

p1<-ggplot(data = a, aes(x = as.factor(Nvar), y = RSQ))+
  geom_point(color="black", size=2) +ylab("RSQ") + xlab("No of variables")

p2<-ggplot(data = a, aes(x = as.factor(Nvar), y = ADJ_RSQ)) +
  geom_point(color="black", size=2) +ylab("Adjusted RSQ") + xlab("No of variables")+
  geom_point(data = a[which.max(a$ADJ_RSQ), ], color="red", 
             size=3) 

p3<-ggplot(data = a, aes(x = as.factor(Nvar), y = CP)) +
  geom_point(color="black", size=2) +ylab("Cp") + xlab("No of variables")+
  geom_point(data = a[which.min(a$CP), ], color="red", 
             size=3) 

p4<-ggplot(data = a, aes(x = as.factor(Nvar), y = BIC)) +
  geom_point(color="black", size=2) +ylab("BIC") + xlab("No of variables")+
  geom_point(data = a[which.min(a$BIC), ], color="red", 
             size=3) 
grid.arrange(p1,p2,p3,p4,nrow=2)


set.seed(123)
sample <- sample(c(TRUE, FALSE), nrow(df2), replace=TRUE, prob=c(0.7,0.3))
train  <- df2[sample, ]
test   <- df2[!sample, ]
train.mat <- model.matrix(Tot_Chol ~., data = train)

MSE <- rep(NA, 16)
for(i in 1:16){
  coefi <- coef(subsets, id = i)
  pred <- train.mat[ ,names(coefi)] %*% coefi
  MSE[i] <- mean((train$Tot_Chol - pred)^2, data=train)
}
MSE<-MSE
test.mat <- model.matrix(Tot_Chol ~., data = test)
MSPE <- rep(NA, 16)
for (i in 1:16) {
  coefi <- coef(subsets, id = i)  
  pred <- test.mat[, names(coefi)] %*% coefi  
  MSPE[i] <- mean((test$Tot_Chol - pred)^2)  
}
e<-cbind(a,MSE,MSPE)
e
```

### Interpretations
Adjusted R^2 is very close in most of these models to mid-90's which is good. However, the best models based on CP and BIC are models 13 and 15.

## Evaluate Selected Models
```{r evaluation}
model13<-lm(Tot_Chol~BMI+Age+Exercise+H1+H2+S1+S2,data=df2)
summary(model13)
vif(model13)
model15<-lm(Tot_Chol~BMI+Age+Exercise+Gender+H1+H2+S1+S2,data=df2)
summary(model15)
vif(model15)
```

### Interpretations
There appear to be no problems with VIF for either model. Due to lower MSPE, we will choose model 15.

## Stepwise Regression
```{r stepwise}
null_model <- lm(Tot_Chol ~ 1, data = df2)

step_model <- step(full_model, direction = "both", 
                   scope = list(lower = null_model, upper = full_model), 
                   trace = 1)
summary(step_model)

par(mfrow = c(2, 2))
plot(step_model)
anova(step_model, model15)
```
